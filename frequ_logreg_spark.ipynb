{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "##      IT'S DANGEROUS TO GO    ##       \n",
    "##       ALONE! TAKE THIS.      ##\n",
    "##        ðŸ”¥  ðŸ§™â€â™‚ï¸  ðŸ”¥          ## \n",
    "##            ðŸ—¡ï¸               ##\n",
    "##                              ##  \n",
    "############  ðŸ§  ############### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score , confusion_matrix, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, first, last, lag, lead, when\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "conf = SparkConf().setAppName('yuck').setMaster(\"local[*]\").set(\"spark.driver.memory\", \"5g\").set(\"spark.executor.memory\", \"5g\").set(\"spark.executor.cores\", \"6\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "#Define the schema for the CSV files\n",
    "schema = StructType([\n",
    "    StructField(\"row_num\", IntegerType(), False),\n",
    "    StructField(\"unix_time\", TimestampType(), True),\n",
    "    StructField(\"AQI_Index\", IntegerType(), True),\n",
    "    StructField(\"AQI_Category\", StringType(), True),\n",
    "    StructField(\"AQI_GenPop_Category\", StringType(), True),\n",
    "    StructField(\"AQI_GenPop_Index\", IntegerType(), True),\n",
    "    StructField(\"BEN\", DoubleType(), True),\n",
    "    StructField(\"CO\", DoubleType(), True),\n",
    "    StructField(\"EBE\", DoubleType(), True),\n",
    "    StructField(\"MXY\", DoubleType(), True),\n",
    "    StructField(\"NMHC\", DoubleType(), True),\n",
    "    StructField(\"NO_2\", DoubleType(), True),\n",
    "    StructField(\"NOx\", DoubleType(), True),\n",
    "    StructField(\"OXY\", DoubleType(), True),\n",
    "    StructField(\"O_3\", DoubleType(), True),\n",
    "    StructField(\"PM10\", DoubleType(), True),\n",
    "    StructField(\"PM25\", DoubleType(), True),\n",
    "    StructField(\"PXY\", DoubleType(), False),\n",
    "    StructField(\"SO_2\", DoubleType(), True),\n",
    "    StructField(\"TCH\", DoubleType(), True),\n",
    "    StructField(\"TOL\", DoubleType(), True)])\n",
    "\n",
    "#insert clean- enormous df csv files to spark_df dataframe\n",
    "data_path = 'C:\\\\Users\\\\eleni\\\\Documents\\\\Diplw\\\\Jupyter-Notebooks\\\\diplw\\\\csvs_per_year\\\\clean_data_norm.csv'\n",
    "spark_df = spark.read.csv(data_path, header=True, schema=schema)\n",
    "#insert training data\n",
    "data = pd.read_csv(\"balanced_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "#Define feature columns\n",
    "#feature_columns = ['BEN','EBE', 'CO', 'NMHC', 'NO_2', 'O_3', 'PM10', 'PM25', 'SO_2','TCH','TOL']\n",
    "feature_columns = [\"NO_2\", \"O_3\", \"PM10\", \"PM25\", \"SO_2\"]\n",
    "\n",
    "#Define target column\n",
    "target_column = \"AQI_GenPop_Index\"\n",
    "\n",
    "#Convert pandas DataFrame to Spark DataFrame\n",
    "data_spark = spark.createDataFrame(data)\n",
    "\n",
    "#Extract columns from data_spark to train_data\n",
    "train_data = data_spark.select(feature_columns + [target_column])\n",
    "\n",
    "#Extract columns from spark_df to test_data\n",
    "test_data = spark_df.select(feature_columns+[target_column])\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "train_data = train_data.withColumn(\"AQI_GenPop_Index\", col(\"AQI_GenPop_Index\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize feature columns\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "train_data = assembler.transform(train_data)\n",
    "test_data = assembler.transform(test_data)\n",
    "\n",
    "#LogisticRegression object\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='AQI_GenPop_Index')\n",
    "\n",
    "#Fit the model to the training data\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "threshold=0.505\n",
    "#Predictions with threhold\n",
    "predictions = model.transform(test_data, {lr.threshold: threshold})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "#Create an evaluator for binary classification\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"AQI_GenPop_Index\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "#Evaluate the model on the test data\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "# Calculate the true positives, false positives, true negatives, and false negatives WHERE positive=hazardous, negative=safe(=not hazardous)\n",
    "tp = predictions.where((predictions[\"AQI_GenPop_Index\"] == 1) & (predictions[\"prediction\"] == 1)).count()\n",
    "fp = predictions.where((predictions[\"AQI_GenPop_Index\"] == 0) & (predictions[\"prediction\"] == 1)).count()\n",
    "tn = predictions.where((predictions[\"AQI_GenPop_Index\"] == 0) & (predictions[\"prediction\"] == 0)).count()\n",
    "fn = predictions.where((predictions[\"AQI_GenPop_Index\"] == 1) & (predictions[\"prediction\"] == 0)).count()\n",
    "\n",
    "#Calculate the accuracy, precision, and recall\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0.0\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "recall_spec= tn /(tn + fp) if (tn+fp) >0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒˆFrequentist Logistic Regression in SparkðŸŒˆ\n",
      " same train and test sets as before(5 features).\n",
      "\n",
      "Test Accuracy: 0.892364262186258\n",
      "Test Precision: 0.9270058241129475\n",
      "Test Recall/Specificity: 0.945249522794354\n",
      "Test AUC ROC: 0.961498458128979\n",
      "\n",
      "    âœ¨Confusion matrixâœ¨\n",
      "TP: 1440301 \t FN: 296489 \n",
      "\n",
      "FP: 113412 \t TN: 1958022\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸŒˆFrequentist Logistic Regression in SparkðŸŒˆ\\n same train and test sets as before(5 features).\\n\")\n",
    "print('Test Accuracy:', accuracy)\n",
    "print('Test Precision:', precision)\n",
    "print('Test Recall:', recall)\n",
    "print('Test Recall/Specificity:', recall_spec)\n",
    "print('Test AUC ROC:', auc)\n",
    "print('\\n    âœ¨Confusion matrixâœ¨')\n",
    "print('TP:',tp,'\\t','FN:',fn,'\\n')\n",
    "print('FP:',fp,'\\t','TN:',tn,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
