{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries as needed\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, first, last, lag, lead, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "conf = SparkConf().setAppName('yuck').setMaster(\"local[*]\").set(\"spark.driver.memory\", \"4g\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# Define the schema for the CSV files\n",
    "schema = StructType([\n",
    "    StructField(\"unix_time\", TimestampType(), True),\n",
    "    StructField(\"row_num\", IntegerType(), False),\n",
    "    StructField(\"BEN\", DoubleType(), True),\n",
    "    StructField(\"CO\", DoubleType(), True),\n",
    "    StructField(\"EBE\", DoubleType(), True),\n",
    "    StructField(\"MXY\", DoubleType(), True),\n",
    "    StructField(\"NMHC\", DoubleType(), True),\n",
    "    StructField(\"NO_2\", DoubleType(), True),\n",
    "    StructField(\"NOx\", DoubleType(), True),\n",
    "    StructField(\"OXY\", DoubleType(), True),\n",
    "    StructField(\"O_3\", DoubleType(), True),\n",
    "    StructField(\"PM10\", DoubleType(), True),\n",
    "    StructField(\"PM25\", DoubleType(), True),\n",
    "    StructField(\"PXY\", DoubleType(), False),\n",
    "    StructField(\"SO_2\", DoubleType(), True),\n",
    "    StructField(\"TCH\", DoubleType(), True),\n",
    "    StructField(\"TOL\", DoubleType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert csv files to spark_df dataframe\n",
    "data_path = 'C:\\\\Users\\\\eleni\\\\Documents\\\\Diplw\\\\Jupyter-Notebooks\\\\diplw\\\\csvs_per_year\\\\yuck\\\\clean_data.csv'\n",
    "spark_df = spark.read.csv(data_path, header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define the breakpoints and corresponding index levels for each pollutant\n",
    "breakpoints = {\n",
    "    'PM25': [0, 10, 20, 25, 50, 75, 800],\n",
    "    'PM10': [0, 20, 40, 50, 100, 150, 1200],\n",
    "    'NO_2': [0, 40, 90, 120, 230, 340, 1000],\n",
    "    'O_3': [0, 50, 100, 130, 240, 380, 800],\n",
    "    'SO_2': [0, 100, 200, 350, 500, 750, 1250]\n",
    "}\n",
    "\n",
    "# Define each category\n",
    "categories = ['Good', 'Fair', 'Moderate', 'Poor', 'Very Poor', 'Extremely Poor']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the index level for each pollutant concentration\n",
    "from pyspark.sql.functions import array, array_max\n",
    "#import pyspark.sql.functions as F\n",
    "\n",
    "def calculate_index_level(pollutant, concentration):\n",
    "    breakpoints_list = breakpoints[pollutant]\n",
    "    for i in range(len(breakpoints_list)-1):\n",
    "        if breakpoints_list[i] <= concentration < breakpoints_list[i+1]:\n",
    "            return i+1\n",
    "    return 6  # If concentration exceeds the highest breakpoint, return the highest index level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "calculate_aqi_index_udf = udf(\n",
    "    lambda no2_conc, o3_conc, pm10_conc, pm25_conc, so2_conc:\n",
    "        int(max([calculate_index_level('NO_2', no2_conc),\n",
    "                 calculate_index_level('O_3', o3_conc),\n",
    "                 calculate_index_level('PM10', pm10_conc),\n",
    "                 calculate_index_level('PM25', pm25_conc),\n",
    "                 calculate_index_level('SO_2', so2_conc)])),\n",
    "    IntegerType()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 2 1 1\n"
     ]
    }
   ],
   "source": [
    "#debug\n",
    "print(calculate_index_level('PM25', 1.77),\n",
    "calculate_index_level('PM10', 23),\n",
    "calculate_index_level('NO_2', 54),\n",
    "calculate_index_level('O_3', 5),\n",
    "calculate_index_level('SO_2', 1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----+----+----+---------+\n",
      "|NO_2|O_3|PM10|PM25|SO_2|AQI_Index|\n",
      "+----+---+----+----+----+---------+\n",
      "|54.0|5.0|23.0|1.77| 1.5|        2|\n",
      "+----+---+----+----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define a sample row to test the UDF\n",
    "sample_row = [(54.0, 5.0, 23.0, 1.77, 1.5)]\n",
    "\n",
    "# Convert the row to a Spark DataFrame\n",
    "sample_df = spark.createDataFrame(sample_row, ['NO_2', 'O_3', 'PM10', 'PM25', 'SO_2'])\n",
    "\n",
    "# Apply the UDF to the sample DataFrame\n",
    "sample_df = sample_df.withColumn('AQI_Index', calculate_aqi_index_udf('NO_2', 'O_3', 'PM10', 'PM25', 'SO_2'))\n",
    "\n",
    "# Show the results\n",
    "sample_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add AQI index as a new column to the DataFrame using the UDF\n",
    "spark_df = spark_df.withColumn('AQI_Index', calculate_aqi_index_udf('NO_2','O_3','PM10', 'PM25', 'SO_2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to calculate the AQI category for each row\n",
    "calculate_aqi_category_udf = udf(lambda index_level: categories[index_level-1], StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add AQI category as a new column to the DataFrame using the UDF\n",
    "spark_df = spark_df.withColumn('AQI_Category', calculate_aqi_category_udf('AQI_Index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-----------------+-------------------+------------------+-----------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+-----------------+-----------+---------+------------+\n",
      "|          unix_time|row_num|              BEN|                 CO|               EBE|              MXY|               NMHC|              NO_2|               NOx|               OXY|               O_3|              PM10|              PM25|              PXY|              SO_2|              TCH|        TOL|AQI_Index|AQI_Category|\n",
      "+-------------------+-------+-----------------+-------------------+------------------+-----------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+-----------------+-----------+---------+------------+\n",
      "|2001-01-01 01:00:00|      1|1.605000078678131| 0.9300000071525574|1.7899999618530273|4.449999809265137| 0.1599999964237213| 54.08000183105469|             103.5|2.1600000858306885| 5.889999866485596|23.229999542236328|1.7799999713897705|7.462675018624933|1.5199999809265137|5.829999923706055|2.8079035E7|        2|        Fair|\n",
      "|2001-01-01 01:00:00|      2|1.605000078678131| 0.9200000166893005| 2.174999952316284|4.514999866485596|0.19999999552965164|             60.25| 99.38999938964844| 2.190000057220459|              5.75|  50.7400016784668|1.7899999618530273|7.462675018624933| 1.550000011920929|5.829999923706055|2.8079003E7|        4|        Poor|\n",
      "|2001-01-01 01:00:00|      3|1.605000078678131| 1.0499999523162842| 2.174999952316284|4.514999866485596|0.19999999552965164|  50.2400016784668| 132.6999969482422| 2.190000057220459| 5.980000019073486|19.649999618530273|1.7899999618530273|7.462675018624933| 1.550000011920929|5.829999923706055|2.8079004E7|        2|        Fair|\n",
      "|2001-01-01 01:00:00|      4|1.605000078678131| 0.7400000095367432| 2.174999952316284|4.514999866485596|0.19999999552965164| 49.40999984741211|             70.75| 2.190000057220459| 2.490000009536743| 36.81999969482422|1.7899999618530273|7.462675018624933| 1.550000011920929|5.829999923706055|2.8079039E7|        2|        Fair|\n",
      "|2001-01-01 01:00:00|      5|1.605000078678131| 1.0900000035762787|1.6150000095367432|4.514999866485596|0.25999999046325684|58.400001525878906|             168.0| 4.489999771118164| 6.769999980926514|25.670000076293945|3.2899999618530273|7.462675018624933|1.5099999904632568|5.829999923706055|2.8079006E7|        2|        Fair|\n",
      "|2001-01-01 01:00:00|      6|1.605000078678131| 0.9100000262260437| 2.174999952316284|4.514999866485596|0.09000000357627869|46.470001220703125| 74.91999816894531| 2.190000057220459|10.140000343322754|22.420000076293945|1.7899999618530273|7.462675018624933|1.2899999618530273|5.829999923706055|2.8079007E7|        2|        Fair|\n",
      "|2001-01-01 01:00:00|      7|1.605000078678131|0.44999998807907104| 2.380000114440918|4.514999866485596|0.19999999552965164| 67.12000274658203|132.89999389648438| 2.190000057220459| 7.860000133514404|32.349998474121094|1.7899999618530273|7.462675018624933| 1.550000011920929|5.829999923706055|2.8079008E7|        2|        Fair|\n",
      "|2001-01-01 01:00:00|      8|1.605000078678131| 1.0900000035762787| 2.174999952316284|4.514999866485596|0.19999999552965164|  57.9900016784668|180.10000610351562| 2.190000057220459|10.430000305175781|22.649999618530273|1.7899999618530273|7.462675018624933| 1.550000011920929|5.829999923706055|2.8079009E7|        2|        Fair|\n",
      "|2001-01-01 01:00:00|      9|1.605000078678131| 1.0900000035762787| 2.174999952316284|4.514999866485596|0.19999999552965164| 53.81999969482422|123.69999694824219| 2.190000057220459| 6.409999847412109|26.540000915527344|1.7899999618530273|7.462675018624933| 1.550000011920929|5.829999923706055|2.8079038E7|        2|        Fair|\n",
      "|2001-01-01 01:00:00|     10|1.605000078678131| 0.7200000286102295| 2.174999952316284|4.514999866485596|0.19999999552965164| 49.83000183105469| 74.04000091552734| 2.190000057220459| 6.369999885559082|22.170000076293945|1.7899999618530273|7.462675018624933| 1.550000011920929|5.829999923706055|2.8079011E7|        2|        Fair|\n",
      "+-------------------+-------+-----------------+-------------------+------------------+-----------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+-----------------+-----------+---------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.write.format('csv').option('header', True ).mode('overwrite').save('C:/Users/eleni/Documents/Diplw/Jupyter-Notebooks/diplw/csvs_per_year/yuck/clean_data_aqi_cat.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
