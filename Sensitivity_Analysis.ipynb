{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pymc3 as pm\n",
    "from sklearn.metrics import accuracy_score , confusion_matrix, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import theano.tensor as tt\n",
    "import arviz as az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"balanced_sample.csv\")\n",
    "#For training and testing only on the small df\n",
    "#feature_columns = ['BEN','EBE', 'CO', 'NMHC', 'NO_2', 'O_3', 'PM10', 'PM25', 'SO_2','TCH','TOL'] #11 features - Best\n",
    "feature_columns = [\"NO_2\", \"O_3\", \"PM10\", \"PM25\", \"SO_2\"] #testing with less \n",
    "X = data[feature_columns].values\n",
    "\n",
    "# Target variable y\n",
    "target_column = \"AQI_GenPop_Index\" \n",
    "y = data[target_column].values\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "n_classes=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as AQI_model:\n",
    "    # Priors for coefficients and bias, with better starting values\n",
    "    coeffs = pm.Normal(\"coeffs\", mu=0, sigma=1, shape=n_features, testval=np.zeros((n_features)))\n",
    "    bias = pm.Normal(\"bias\", mu=0, sigma=1)\n",
    "\n",
    "def logistic(x, epsilon=1e-6):\n",
    "    return 1 / (1 + tt.exp(-x)) + epsilon\n",
    "  \n",
    "    p = logistic(pm.math.dot(X_train, coeffs) + bias)\n",
    "    # Define the Bernoulli likelihood\n",
    "    y_obs = pm.Bernoulli(\"y_obs\", p=p, observed=y_train)    \n",
    "# MCMC\n",
    "with AQI_model:\n",
    "    #step=pm.Metropolis()\n",
    "    #step=pm.NUTS(target_accept=0.8)\n",
    "    #n_init only for advi methods, default 20000\n",
    "    trace = pm.sample(2000, tune=1600, chains=8, cores=4, return_inferencedata=False )#,step=step, \n",
    "    sns.set_palette(\"BuPu\")\n",
    "    pm.plot_trace(trace)\n",
    "\n",
    "az.summary(trace)\n",
    "sns.set_palette(\"BuPu_r\")\n",
    "with AQI_model:\n",
    "    az.plot_trace(trace, compact=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BINARY Predicting on test data\n",
    "def predict_proba(X, trace):\n",
    "    linear = np.dot(X, trace[\"coeffs\"].mean(axis=0)) + trace[\"bias\"].mean()\n",
    "    proba = 1 / (1 + np.exp(-linear))\n",
    "    return np.column_stack((1 - proba, proba)) #(proba negative(0), proba positve(1))\n",
    "y_test_pred_proba = predict_proba(X_test, trace)\n",
    "y_test_pred = np.argmax(y_test_pred_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of model performance\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test accuracy:\", accuracy)\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_test_pred, average='macro') #try also 'micro' or 'weighted'\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_score= roc_auc_score(y_test, y_test_pred)\n",
    "print('ROC AUC:', roc_score) #roc_score<0.5-->poor classifier, =0.5-->random classifier, \n",
    "                             #>0.7-->good classifier, 0.8->strong, 1->best"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
