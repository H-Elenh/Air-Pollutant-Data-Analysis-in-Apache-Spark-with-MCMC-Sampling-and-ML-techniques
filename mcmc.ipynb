{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc3 as pm\n",
    "from sklearn.metrics import accuracy_score , confusion_matrix, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import theano.tensor as tt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"balanced_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the columns to plot\n",
    "pollutants = data.columns[4:] \n",
    "\n",
    "# Create a figure with 5x3 subplots\n",
    "fig, axs = plt.subplots(nrows=5, ncols=3, figsize=(16, 16))\n",
    "\n",
    "# Plot a histogram for each column\n",
    "for i, pollutant in enumerate(pollutants):\n",
    "    \n",
    "    # Get the data for the column\n",
    "    dat = data[pollutant].dropna()\n",
    "    \n",
    "    # Determine the subplot location based on the column index\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    \n",
    "    # get the range of values in the column, ignoring NaN and Inf\n",
    "    x_min = np.nanmin(data[pollutant][np.isfinite(data[pollutant])])\n",
    "    x_max = np.nanmax(data[pollutant][np.isfinite(data[pollutant])])\n",
    "    \n",
    "    # calculate number of bins using IQR rule\n",
    "    n = len(data[pollutant])\n",
    "    std = np.std(data[pollutant])\n",
    "    k = 3.5 * std / (n**(1/3))\n",
    "    num_bins = int((x_max - x_min) / k) if k != 0 else 1 # Added this line to handle the case when k=0\n",
    "    \n",
    "    # Plot the histogram with kde\n",
    "    sns.histplot(data[pollutant], kde=True, bins=num_bins, color=sns.color_palette(\"PuRd\", 15)[i], ax=axs[row, col])\n",
    "    \n",
    "    # Add a vertical line for the mean\n",
    "    mean = data[pollutant].mean()\n",
    "    axs[row, col].axvline(mean, color='k', linestyle='dashed', linewidth=1)\n",
    "    \n",
    "    # Add a vertical line for the standard deviation\n",
    "    std = data[pollutant].std()\n",
    "    axs[row, col].axvline(mean+std, color='#8C78F0', linestyle='dashed', linewidth=1)\n",
    "    axs[row, col].axvline(mean-std, color='#8C78F0', linestyle='dashed', linewidth=1)\n",
    "    \n",
    "    # Set the title and axis labels\n",
    "    axs[row, col].set_title(pollutant)\n",
    "    axs[row, col].set_xlabel('Value')\n",
    "    axs[row, col].set_ylabel('Density')\n",
    "    axs[row, col].set_xlim([dat.min(), dat.max()])  # set x-axis range\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the color palette to PuRd\n",
    "sns.set_palette('BuPu',2)\n",
    "\n",
    "# Create bar chart of the categorical variable\n",
    "data['AQI_GenPop_Index'].value_counts().plot(kind='bar')\n",
    "\n",
    "# Create bar chart of the categorical variable\n",
    "#data['AQI_Index'].value_counts().plot(kind='bar')\n",
    "\n",
    "# Set plot title and axis labels\n",
    "plt.title('Distribution of my categorical variable')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['BEN','EBE', 'CO', 'NMHC', 'NO_2', 'O_3', 'PM10', 'PM25', 'SO_2','TCH','TOL'] #11 features - Best\n",
    "\n",
    "#feature_columns = [\"NO_2\", \"O_3\", \"PM10\", \"PM25\", \"SO_2\"] #testing with less //not so good\n",
    "\n",
    "#feature_columns = ['BEN','EBE', 'CO', 'NMHC', 'NO_2','NOx', 'O_3', 'PM10', 'PM25', 'SO_2','TCH', 'TOL','MXY','OXY','PXY'] #15features to test with more features -NOT BAD\n",
    "\n",
    "X = data[feature_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the target variable y\n",
    "target_column = \"AQI_GenPop_Index\" \n",
    "y = data[target_column].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "#n_classes = len(np.unique(y_train)) #for Multi Class LogReg\n",
    "n_classes=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View data mean and std\n",
    "mean = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "\n",
    "print(f\"Mean of features: {mean}\")\n",
    "print(f\"Std of features: {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GEN_POP - Build the Binary Bayesian logistic regression model in PyMC3\n",
    "with pm.Model() as AQI_model:\n",
    "    # Priors for coefficients and bias, with better starting values\n",
    "    coeffs = pm.Normal(\"coeffs\", mu=0, sigma=1, shape=n_features, testval=np.zeros((n_features)))\n",
    "    bias = pm.Normal(\"bias\", mu=0, sigma=1)\n",
    "    \n",
    "    #prin to eixes ektos tou modelou kai kapws douleve kalutera\n",
    "# Define the logistic function with added epsilon\n",
    "def logistic(x, epsilon=1e-6):\n",
    "    return 1 / (1 + tt.exp(-x)) + epsilon\n",
    "\n",
    "    p = pm.math.sigmoid(pm.math.dot(X_train, coeffs) + bias)\n",
    "\n",
    "    # Define the Bernoulli likelihood\n",
    "    y_obs = pm.Bernoulli(\"y_obs\", p=p, observed=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCMC\n",
    "with AQI_model:\n",
    "    #step= pm.Metropolis()\n",
    "    step=pm.NUTS(target_accept=0.8)\n",
    "    #step=pm.HamiltonianMC()\n",
    "    trace = pm.sample(10000, tune=1000, chains=8, cores=8, step=step, progressbar=True)\n",
    "    sns.set_palette(\"BuPu\")\n",
    "    pm.plot_trace(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with AQI_model:\n",
    "    print(pm.summary(trace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BINARY Predicting on test data\n",
    "\n",
    "def predict_proba(X, trace):\n",
    "    linear = np.dot(X, trace[\"coeffs\"].mean(axis=0)) + trace[\"bias\"].mean()\n",
    "    proba = 1 / (1 + np.exp(-linear))\n",
    "    return np.column_stack((1 - proba, proba)) #this code is the original code\n",
    "\n",
    "y_test_pred_proba = predict_proba(X_test, trace)\n",
    "y_test_pred = np.argmax(y_test_pred_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"Confusion matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Evaluation of model performance\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test accuracy:\", accuracy)\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_test_pred, average='macro') #try also 'micro' or 'weighted'\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiclass bad accuracies //AYTA POU EIXA MIN TA PEIRAKSEIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Bayesian logistic regression model in PyMC3\n",
    "with pm.Model() as AQI_model:\n",
    "    # Priors for coefficients and bias, with better starting values\n",
    "    coeffs = pm.Normal(\"coeffs\", mu=0, sigma=1, shape=(n_features, n_classes - 1), testval=np.zeros((n_features, n_classes - 1)))\n",
    "    bias = pm.Normal(\"bias\", mu=0, sigma=1, shape=(n_classes - 1,), testval=np.zeros(n_classes - 1))\n",
    "\n",
    "    # Define the softmax function using Theano functions with added epsilon for numerical stability\n",
    "    def softmax(x, epsilon=1e-6):\n",
    "        e_x = tt.exp(x - tt.max(x, axis=1, keepdims=True))\n",
    "        e_x += epsilon\n",
    "        return e_x / tt.sum(e_x, axis=1, keepdims=True)\n",
    "\n",
    "        # Likelihood function (using the custom softmax function for multi-class classification)\n",
    "        linear = pm.math.dot(X_train, coeffs) + bias\n",
    "        softmax_probs = softmax(linear)\n",
    "\n",
    "        # Define the categorical likelihood\n",
    "        y_obs = pm.Categorical(\"y_obs\", p=softmax_probs, observed=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCMC\n",
    "with AQI_model:\n",
    "    #step= pm.Metropolis()\n",
    "    step=pm.NUTS(target_accept=0.8)\n",
    "    trace = pm.sample(8000, tune=800, chains=8, cores=8, step=step, progressbar=True)\n",
    "    pm.plot_trace(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with AQI_model:\n",
    "    pm.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MULTI-CLASS:Predicting on test data\n",
    "def predict_proba(X, trace):\n",
    "  linear = np.dot(X, trace[\"coeffs\"].mean(axis=0)) + trace[\"bias\"].mean(axis=0)\n",
    "  softmax = np.exp(linear - np.max(linear, axis=1, keepdims=True))\n",
    "  return softmax / np.sum(softmax, axis=1, keepdims=True)\n",
    "\n",
    "y_test_pred_proba = predict_proba(X_test, trace)\n",
    "y_test_pred = np.argmax(y_test_pred_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"Confusion matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of model performance\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_test_pred, average='macro') #try also 'micro' or 'weighted'\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision_micro = precision_score(y_test, y_test_pred, average='micro')\n",
    "recall_micro = recall_score(y_test, y_test_pred, average='micro')\n",
    "\n",
    "print(\"Micro-averaged precision:\", precision_micro)\n",
    "print(\"Micro-averaged recall:\", recall_micro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision_macro = precision_score(y_test, y_test_pred, average='macro')\n",
    "recall_macro = recall_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "print(\"Macro-averaged precision:\", precision_macro)\n",
    "print(\"Macro-averaged recall:\", recall_macro)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
