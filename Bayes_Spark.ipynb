{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "##      IT'S DANGEROUS TO GO    ##       \n",
    "##       ALONE! TAKE THIS.      ##\n",
    "##        ðŸ”¥  ðŸ§™â€â™‚ï¸  ðŸ”¥          ## \n",
    "##            ðŸ—¡ï¸               ##\n",
    "##                              ##  \n",
    "############  ðŸ§  ############### \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score , confusion_matrix, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import theano.tensor as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"balanced_sample.csv\")\n",
    "#FEATURES\n",
    "#feature_columns = ['BEN','EBE', 'CO', 'NMHC', 'NO_2', 'O_3', 'PM10', 'PM25', 'SO_2','TCH','TOL'] #11 features - Best\n",
    "feature_columns = [\"NO_2\", \"O_3\", \"PM10\", \"PM25\", \"SO_2\"]\n",
    "X_train = data[feature_columns].values\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "#TARGET\n",
    "target_column = \"AQI_GenPop_Index\" \n",
    "y_train = data[target_column].values\n",
    "n_classes=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as AQI_model:\n",
    "    # Priors for coefficients and bias, with better starting values\n",
    "    coeffs = pm.Normal(\"coeffs\", mu=0, sigma=1, shape=n_features, testval=np.zeros((n_features)))\n",
    "    bias = pm.Normal(\"bias\", mu=0, sigma=1)\n",
    "    \n",
    "    # Likelihood function\n",
    "    # Define the logistic function with added epsilon\n",
    "def logistic(x, epsilon=1e-6):\n",
    "    return 1 / (1 + tt.exp(-x)) + epsilon\n",
    "       \n",
    "    p = logistic(pm.math.dot(X_train, coeffs) + bias)\n",
    "    \n",
    "    # Define the Bernoulli likelihood\n",
    "    y_obs = pm.Bernoulli(\"y_obs\", p=p, observed=y_train)\n",
    "# MCMC\n",
    "with AQI_model:\n",
    "    #step=pm.Metropolis()\n",
    "    trace = pm.sample(1000, tune=2000, chains=4, cores=8, init='advi_map', n_init=100000, progressbar=True)\n",
    "    sns.set_palette(\"BuPu\")\n",
    "    pm.plot_trace(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, first, last, lag, lead, when\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "conf = SparkConf().setAppName('yuck').setMaster(\"local[*]\").set(\"spark.driver.memory\", \"5g\").set(\"spark.executor.memory\", \"5g\").set(\"spark.executor.cores\", \"6\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "#Define the schema for the CSV files\n",
    "schema = StructType([\n",
    "    StructField(\"row_num\", IntegerType(), False),\n",
    "    StructField(\"unix_time\", TimestampType(), True),\n",
    "    StructField(\"AQI_Index\", IntegerType(), True),\n",
    "    StructField(\"AQI_Category\", StringType(), True),\n",
    "    StructField(\"AQI_GenPop_Category\", StringType(), True),\n",
    "    StructField(\"AQI_GenPop_Index\", IntegerType(), True),\n",
    "    StructField(\"BEN\", DoubleType(), True),\n",
    "    StructField(\"CO\", DoubleType(), True),\n",
    "    StructField(\"EBE\", DoubleType(), True),\n",
    "    StructField(\"MXY\", DoubleType(), True),\n",
    "    StructField(\"NMHC\", DoubleType(), True),\n",
    "    StructField(\"NO_2\", DoubleType(), True),\n",
    "    StructField(\"NOx\", DoubleType(), True),\n",
    "    StructField(\"OXY\", DoubleType(), True),\n",
    "    StructField(\"O_3\", DoubleType(), True),\n",
    "    StructField(\"PM10\", DoubleType(), True),\n",
    "    StructField(\"PM25\", DoubleType(), True),\n",
    "    StructField(\"PXY\", DoubleType(), False),\n",
    "    StructField(\"SO_2\", DoubleType(), True),\n",
    "    StructField(\"TCH\", DoubleType(), True),\n",
    "    StructField(\"TOL\", DoubleType(), True)])\n",
    "\n",
    "# insert clean- enormous df csv files to spark_df dataframe\n",
    "data_path = 'C:\\\\Users\\\\eleni\\\\Documents\\\\Diplw\\\\Jupyter-Notebooks\\\\diplw\\\\csvs_per_year\\\\clean_data_norm.csv'\n",
    "spark_df = spark.read.csv(data_path, header=True, schema=schema)\n",
    "#Extract the features from the test data\n",
    "X_test = spark_df.select(feature_columns).rdd.map(list).collect()\n",
    "#Extract labels from the test data\n",
    "y_test = spark_df.select(target_column).rdd.map(list).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import exp\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "@udf(returnType=ArrayType(DoubleType()))\n",
    "def predict_proba_udf(coeffs_list, bias_value, *features):\n",
    "    linear = sum([features[i] * coeffs_list[i] for i in range(len(coeffs_list))]) + bias_value\n",
    "    proba = 1 / (1 + math.exp(-linear))\n",
    "    return [1 - proba, proba]\n",
    "\n",
    "from pyspark.sql.functions import col, array, lit\n",
    "\n",
    "#Handle trace coeffs and bias for manipulation in spark\n",
    "coeffs = trace[\"coeffs\"].mean(axis=0)\n",
    "bias = trace[\"bias\"].mean()\n",
    "\n",
    "#ðŸ§šâ€â™‚ï¸âœ¨Hey, listenâœ¨ðŸ§šâ€â™‚ï¸\n",
    "#coeffs and bias are NumPy arrays -> we can't pass NumPy arrays to a Spark UDF. \n",
    "#Solution = convert arrays to list before passing them to the UDF\n",
    "\n",
    "coeffs_list = coeffs.tolist()\n",
    "bias_value = float(bias)\n",
    "# Convert coeffs_list to a list of Column objects\n",
    "coeffs_columns = [lit(x) for x in coeffs_list]\n",
    "\n",
    "# Create a column for the coefficients array\n",
    "coeffs_array = array(*coeffs_columns)\n",
    "\n",
    "spark_df = spark_df.withColumn(\"proba\", predict_proba_udf(coeffs_array, lit(bias_value), *[col(c) for c in feature_columns]))\n",
    "spark_df = spark_df.withColumn(\"y_pred\", (col(\"proba\")[1] >= 0.50).cast(DoubleType())) #try 0.5,0.499999, 0.49785 etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, when, col\n",
    "\n",
    "# Calculate the true positives, false positives, true negatives, and false negatives WHERE positive=hazardous, negative=safe(=not hazardous)\n",
    "tp = spark_df.where((spark_df[target_column] == 1) & (spark_df[\"y_pred\"] == 1)).count()\n",
    "fp = spark_df.where((spark_df[target_column] == 0) & (spark_df[\"y_pred\"] == 1)).count()\n",
    "tn = spark_df.where((spark_df[target_column] == 0) & (spark_df[\"y_pred\"] == 0)).count()\n",
    "fn = spark_df.where((spark_df[target_column] == 1) & (spark_df[\"y_pred\"] == 0)).count()\n",
    "\n",
    "# Calculate the accuracy, precision, and recall\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0.0\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "recall_spec= tn /(tn + fp) if (tn+fp) >0 else 0.0\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Create evaluator object\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"y_pred\", labelCol=\"AQI_GenPop_Index\")\n",
    "\n",
    "# Compute AUC ROC score\n",
    "auc_roc = evaluator.evaluate(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#âœ¨Check if unbiasedâœ¨\n",
    "#Logistic regression predictions should be unbiased. That is: \"average of predictions\" should â‰ˆ \"average of observations\n",
    "\n",
    "from pyspark.sql.functions import mean, format_number\n",
    "\n",
    "# Calculate the mean of y_pred and AQI_GenPop_Index columns\n",
    "means = spark_df.agg(*[mean(c).alias(c) for c in ['y_pred', 'AQI_GenPop_Index']])\n",
    "\n",
    "# Format the mean values to 4 decimal places\n",
    "formatted_means = means.select(*[format_number(c, 4).alias(c) for c in means.columns])\n",
    "\n",
    "# Show the formatted mean values\n",
    "formatted_means.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
