{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries as needed\n",
    "import os\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score , confusion_matrix, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, first, last, lag, lead, when\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "conf = SparkConf().setAppName('yuck').setMaster(\"local[*]\").set(\"spark.driver.memory\", \"5g\").set(\"spark.executor.memory\", \"5g\").set(\"spark.executor.cores\", \"6\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR BIG DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import unix_timestamp\n",
    "#from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# Define the schema for the CSV files\n",
    "#schema = StructType([\n",
    "#    StructField(\"row_num\", IntegerType(), False),\n",
    "#    StructField(\"unix_time\", TimestampType(), True),\n",
    "#     StructField(\"AQI_Index\", IntegerType(), True),\n",
    "#     StructField(\"AQI_Category\", StringType(), True),\n",
    "#     StructField(\"AQI_GenPop_Category\", StringType(), True),\n",
    "#     StructField(\"AQI_GenPop_Index\", IntegerType(), True),\n",
    "#     StructField(\"BEN\", DoubleType(), True),\n",
    "#     StructField(\"CO\", DoubleType(), True),\n",
    "#     StructField(\"EBE\", DoubleType(), True),\n",
    "#     StructField(\"MXY\", DoubleType(), True),\n",
    "#     StructField(\"NMHC\", DoubleType(), True),\n",
    "#     StructField(\"NO_2\", DoubleType(), True),\n",
    "#     StructField(\"NOx\", DoubleType(), True),\n",
    "#     StructField(\"OXY\", DoubleType(), True),\n",
    "#     StructField(\"O_3\", DoubleType(), True),\n",
    "#     StructField(\"PM10\", DoubleType(), True),\n",
    "#     StructField(\"PM25\", DoubleType(), True),\n",
    "#     StructField(\"PXY\", DoubleType(), False),\n",
    "#     StructField(\"SO_2\", DoubleType(), True),\n",
    "#     StructField(\"TCH\", DoubleType(), True),\n",
    "#     StructField(\"TOL\", DoubleType(), True)])\n",
    "# insert clean- enormous df csv files to spark_df dataframe\n",
    "# data_path = 'C:\\\\Users\\\\eleni\\\\Documents\\\\Diplw\\\\Jupyter-Notebooks\\\\diplw\\\\csvs_per_year\\\\clean_data_norm.csv'\n",
    "# data = spark.read.csv(data_path, header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Play around with small df first\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# Define the schema for the CSV files\n",
    "schema = StructType([\n",
    "    StructField(\"AQI_Index\", IntegerType(), True),\n",
    "    StructField(\"AQI_Category\", StringType(), True),\n",
    "    StructField(\"AQI_GenPop_Category\", StringType(), True),\n",
    "    StructField(\"AQI_GenPop_Index\", IntegerType(), True),\n",
    "    StructField(\"BEN\", DoubleType(), True),\n",
    "    StructField(\"CO\", DoubleType(), True),\n",
    "    StructField(\"EBE\", DoubleType(), True),\n",
    "    StructField(\"MXY\", DoubleType(), True),\n",
    "    StructField(\"NMHC\", DoubleType(), True),\n",
    "    StructField(\"NO_2\", DoubleType(), True),\n",
    "    StructField(\"NOx\", DoubleType(), True),\n",
    "    StructField(\"OXY\", DoubleType(), True),\n",
    "    StructField(\"O_3\", DoubleType(), True),\n",
    "    StructField(\"PM10\", DoubleType(), True),\n",
    "    StructField(\"PM25\", DoubleType(), True),\n",
    "    StructField(\"PXY\", DoubleType(), False),\n",
    "    StructField(\"SO_2\", DoubleType(), True),\n",
    "    StructField(\"TCH\", DoubleType(), True),\n",
    "    StructField(\"TOL\", DoubleType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert 1 year csv file to spark dataframe\n",
    "data_path = 'C:\\\\Users\\\\eleni\\\\Documents\\\\Diplw\\\\Jupyter-Notebooks\\\\diplw\\\\balanced_sample.csv'\n",
    "data = spark.read.csv(data_path, header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECK IF SPARK_DF GOOD TO GO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if df ok\n",
    "from pyspark.sql.functions import col, sum\n",
    "from pyspark.sql.functions import isnan, when, count\n",
    "\n",
    "null_counts = data.agg(*[sum(col(c).isNull().cast(\"int\")).alias(c) for c in data.columns])\n",
    "\n",
    "# Print out the null counts for each column\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pollutants=data.columns[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, format_number\n",
    "\n",
    "# assume 'data' is your Spark DataFrame\n",
    "means = data.agg(*[mean(c).alias(c) for c in data.columns[6:]])\n",
    "# Format the mean values to 4 decimal places\n",
    "formatted_means = means.select(*[format_number(c, 4).alias(c) for c in means.columns])\n",
    "\n",
    "# Show the formatted mean values\n",
    "formatted_means.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import stddev, format_number\n",
    "\n",
    "# assume 'data' is your Spark DataFrame\n",
    "stds = data.agg(*[stddev(c).alias(c) for c in data.columns[6:]])\n",
    "# Format the mean values to 4 decimal places\n",
    "formatted_stvs = stds.select(*[format_number(c, 4).alias(c) for c in stds.columns])\n",
    "\n",
    "# Show the formatted mean values\n",
    "formatted_stvs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BAYESIAN LOGISTIC REGRESSION STARTS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_likelihood(p, y):\n",
    "    return tt.pow(p, y) * tt.pow(1 - p, 1 - y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Bayesian logistic regression model\n",
    "def bayesian_logistic_regression(X_train, y_train, n_features):\n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"y_train shape:\", y_train.shape)\n",
    "    \n",
    "    with pm.Model() as AQI_model:\n",
    "        # Priors for coefficients and bias, with better starting values\n",
    "        coeffs = pm.Normal(\"coeffs\", mu=0, sigma=1, shape=n_features, testval=np.zeros((n_features)))\n",
    "        bias = pm.Normal(\"bias\", mu=0, sigma=1)\n",
    "\n",
    "    def logistic(x, epsilon=1e-6):\n",
    "        return 1 / (1 + tt.exp(-x)) + epsilon   \n",
    "    \n",
    "        # Calculate the probability of each observation using the logistic function\n",
    "        p = logistic(pm.math.sigmoid(pm.math.dot(X_train, coeffs) + bias))\n",
    "\n",
    "        # Define the Bernoulli likelihood\n",
    "        y_obs = pm.Bernoulli(\"y_obs\", p=p, observed=y_train)\n",
    "\n",
    "    with AQI_model:\n",
    "        print(\"Starting MCMC sampling...\")\n",
    "        # Fit the model using the MCMC No-U-Turn Sampler (NUTS)\n",
    "        step=pm.NUTS(target_accept=0.8)\n",
    "        trace = pm.sample(8000, tune=800, chains=4, cores=4, step=step, progressbar=True)\n",
    "\n",
    "        # Return the posterior mean of the coefficients and bias\n",
    "        return {\"coeffs\": np.mean(trace[\"coeffs\"], axis=0), \"bias\": np.mean(trace[\"bias\"])}\n",
    "\n",
    "    \n",
    "bayesian_logistic_regression_udf = udf(bayesian_logistic_regression, returnType=DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=ArrayType(DoubleType()))\n",
    "def predict_proba_udf(X_test, coeffs, bias):\n",
    "    linear = np.dot(X_test, coeffs) + bias\n",
    "    proba = 1 / (1 + np.exp(-linear))\n",
    "    return np.column_stack((1 - proba, proba))\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def predict_udf(X_test, coeffs, bias):\n",
    "    y_test_pred_proba = predict_proba_udf(X_test, coeffs, bias)\n",
    "    y_test_pred = np.argmax(y_test_pred_proba, axis=1)\n",
    "    return y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# Extract the feature columns and target column from the spark dataframe\n",
    "feature_columns = ['BEN', 'EBE', 'CO', 'NMHC', 'NO_2', 'O_3', 'PM10', 'PM25', 'SO_2', 'TCH', 'TOL']\n",
    "n_features = len(feature_columns)\n",
    "target_column = \"AQI_GenPop_Index\"\n",
    "print(n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a VectorAssembler to combine the feature columns into a single feature vector column\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "# Apply the VectorAssembler to your data\n",
    "data = assembler.transform(data.select(feature_columns + [target_column]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the target variable y\n",
    "data = data.select(\"features\", target_column).withColumnRenamed(target_column, \"AQI_GenPop_Index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN AND TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(bayesian_logistic_regression_udf(\"features\", \"AQI_GenPop_Index\", lit(n_features)))\n",
    "train_data.select(\"features\", \"AQI_GenPop_Index\").show()\n",
    "print(\"Input data to UDF: train_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"c:\\Users\\eleni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\theano\\__init__.py\", line 83, in <module>\n    from theano import scalar, tensor\n  File \"c:\\Users\\eleni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\theano\\tensor\\__init__.py\", line 20, in <module>\n    from theano.tensor import nnet  # used for softmax, sigmoid, etc.\n  File \"c:\\Users\\eleni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\theano\\tensor\\nnet\\__init__.py\", line 3, in <module>\n    from . import opt\n  File \"c:\\Users\\eleni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\theano\\tensor\\nnet\\opt.py\", line 32, in <module>\n    from theano.tensor.nnet.conv import ConvOp, conv2d\n  File \"c:\\Users\\eleni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\theano\\tensor\\nnet\\conv.py\", line 20, in <module>\n    from theano.tensor import blas\n  File \"c:\\Users\\eleni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\theano\\tensor\\blas.py\", line 163, in <module>\n    from theano.tensor.blas_headers import blas_header_text, blas_header_version\n  File \"c:\\Users\\eleni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\theano\\tensor\\blas_headers.py\", line 1016, in <module>\n    if not config.blas__ldflags:\n  File \"c:\\Users\\eleni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\theano\\configparser.py\", line 358, in __get__\n    val_str = self.default()\n  File \"c:\\Users\\eleni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\theano\\link\\c\\cmodule.py\", line 2826, in default_blas_ldflags\n    except KeyError:\nAttributeError: module 'numpy.distutils.__config__' has no attribute 'blas_opt_info'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Train the model on the training data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctions\u001b[39;00m \u001b[39mimport\u001b[39;00m col, lit\n\u001b[1;32m----> 4\u001b[0m model_dict \u001b[39m=\u001b[39m train_data\u001b[39m.\u001b[39;49mselect(bayesian_logistic_regression_udf(col(\u001b[39m\"\u001b[39;49m\u001b[39mfeatures\u001b[39;49m\u001b[39m\"\u001b[39;49m), col(\u001b[39m\"\u001b[39;49m\u001b[39mAQI_GenPop_Index\u001b[39;49m\u001b[39m\"\u001b[39;49m), lit(n_features))\u001b[39m.\u001b[39;49malias(\u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m))\u001b[39m.\u001b[39;49mfirst()[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      6\u001b[0m coeffs \u001b[39m=\u001b[39m model_dict[\u001b[39m\"\u001b[39m\u001b[39mcoeffs\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      7\u001b[0m bias \u001b[39m=\u001b[39m model_dict[\u001b[39m\"\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mC:\\spark-3.2.3-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py:1617\u001b[0m, in \u001b[0;36mDataFrame.first\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1607\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfirst\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1608\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns the first row as a :class:`Row`.\u001b[39;00m\n\u001b[0;32m   1609\u001b[0m \n\u001b[0;32m   1610\u001b[0m \u001b[39m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1615\u001b[0m \u001b[39m    Row(age=2, name='Alice')\u001b[39;00m\n\u001b[0;32m   1616\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1617\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhead()\n",
      "File \u001b[1;32mC:\\spark-3.2.3-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py:1603\u001b[0m, in \u001b[0;36mDataFrame.head\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1576\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns the first ``n`` rows.\u001b[39;00m\n\u001b[0;32m   1577\u001b[0m \n\u001b[0;32m   1578\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1600\u001b[0m \u001b[39m[Row(age=2, name='Alice')]\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1602\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1603\u001b[0m     rs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhead(\u001b[39m1\u001b[39;49m)\n\u001b[0;32m   1604\u001b[0m     \u001b[39mreturn\u001b[39;00m rs[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m rs \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1605\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(n)\n",
      "File \u001b[1;32mC:\\spark-3.2.3-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py:1605\u001b[0m, in \u001b[0;36mDataFrame.head\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1603\u001b[0m     rs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead(\u001b[39m1\u001b[39m)\n\u001b[0;32m   1604\u001b[0m     \u001b[39mreturn\u001b[39;00m rs[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m rs \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1605\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtake(n)\n",
      "File \u001b[1;32mC:\\spark-3.2.3-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py:744\u001b[0m, in \u001b[0;36mDataFrame.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m    734\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtake\u001b[39m(\u001b[39mself\u001b[39m, num):\n\u001b[0;32m    735\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\u001b[39;00m\n\u001b[0;32m    736\u001b[0m \n\u001b[0;32m    737\u001b[0m \u001b[39m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[39m    [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[0;32m    743\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 744\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlimit(num)\u001b[39m.\u001b[39;49mcollect()\n",
      "File \u001b[1;32mC:\\spark-3.2.3-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py:693\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    683\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[0;32m    684\u001b[0m \n\u001b[0;32m    685\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[39m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[0;32m    691\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    692\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sc) \u001b[39mas\u001b[39;00m css:\n\u001b[1;32m--> 693\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mcollectToPython()\n\u001b[0;32m    694\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001b[1;32mC:\\spark-3.2.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\spark-3.2.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    113\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    115\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"c:\\Users\\eleni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\theano\\__init__.py\", line 83, in <module>\n    from theano import scalar, tensor\n  File \"c:\\Users\\eleni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\theano\\tensor\\__init__.py\", line 20, in <module>\n    from theano.tensor import nnet  # used for softmax, sigmoid, etc.\n  File \"c:\\Users\\eleni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\theano\\tensor\\nnet\\__init__.py\", line 3, in <module>\n    from . import opt\n  File \"c:\\Users\\eleni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\theano\\tensor\\nnet\\opt.py\", line 32, in <module>\n    from theano.tensor.nnet.conv import ConvOp, conv2d\n  File \"c:\\Users\\eleni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\theano\\tensor\\nnet\\conv.py\", line 20, in <module>\n    from theano.tensor import blas\n  File \"c:\\Users\\eleni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\theano\\tensor\\blas.py\", line 163, in <module>\n    from theano.tensor.blas_headers import blas_header_text, blas_header_version\n  File \"c:\\Users\\eleni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\theano\\tensor\\blas_headers.py\", line 1016, in <module>\n    if not config.blas__ldflags:\n  File \"c:\\Users\\eleni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\theano\\configparser.py\", line 358, in __get__\n    val_str = self.default()\n  File \"c:\\Users\\eleni\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\theano\\link\\c\\cmodule.py\", line 2826, in default_blas_ldflags\n    except KeyError:\nAttributeError: module 'numpy.distutils.__config__' has no attribute 'blas_opt_info'\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the training data\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "model_dict = train_data.select(bayesian_logistic_regression_udf(col(\"features\"), col(\"AQI_GenPop_Index\"), lit(n_features)).alias(\"model\")).first()[\"model\"]\n",
    "\n",
    "coeffs = model_dict[\"coeffs\"]\n",
    "bias = model_dict[\"bias\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the training data\n",
    "coeffs, bias = train_data.select(bayesian_logistic_regression_udf(\"features\", \"AQI_GenPop_Index\", lit(n_features)).alias(\"model\")).first()[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "test_data_pred = test_data.withColumn(\"prediction\", predict_udf(\"features\", lit(coeffs), lit(bias)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of model // WE WANT BINARY\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"AQI_GenPop_Index\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(test_data_pred)\n",
    "\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
