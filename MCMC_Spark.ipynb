{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries as needed\n",
    "import os\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc3 as pm\n",
    "from sklearn.metrics import accuracy_score , confusion_matrix, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import theano.tensor as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, first, last, lag, lead, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "conf = SparkConf().setAppName('yuck').setMaster(\"local[*]\").set(\"spark.driver.memory\", \"4g\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# Define the schema for the CSV files\n",
    "schema = StructType([\n",
    "    StructField(\"row_num\", IntegerType(), False),\n",
    "    StructField(\"unix_time\", TimestampType(), True),\n",
    "    StructField(\"AQI_Index\", IntegerType(), True),\n",
    "    StructField(\"AQI_Category\", StringType(), True),\n",
    "    StructField(\"AQI_GenPop_Category\", StringType(), True),\n",
    "    StructField(\"AQI_GenPop_Index\", IntegerType(), True),\n",
    "    StructField(\"BEN\", DoubleType(), True),\n",
    "    StructField(\"CO\", DoubleType(), True),\n",
    "    StructField(\"EBE\", DoubleType(), True),\n",
    "    StructField(\"MXY\", DoubleType(), True),\n",
    "    StructField(\"NMHC\", DoubleType(), True),\n",
    "    StructField(\"NO_2\", DoubleType(), True),\n",
    "    StructField(\"NOx\", DoubleType(), True),\n",
    "    StructField(\"OXY\", DoubleType(), True),\n",
    "    StructField(\"O_3\", DoubleType(), True),\n",
    "    StructField(\"PM10\", DoubleType(), True),\n",
    "    StructField(\"PM25\", DoubleType(), True),\n",
    "    StructField(\"PXY\", DoubleType(), False),\n",
    "    StructField(\"SO_2\", DoubleType(), True),\n",
    "    StructField(\"TCH\", DoubleType(), True),\n",
    "    StructField(\"TOL\", DoubleType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert csv files to spark_df dataframe\n",
    "data_path = 'C:\\\\Users\\\\eleni\\\\Documents\\\\Diplw\\\\Jupyter-Notebooks\\\\diplw\\\\csvs_per_year\\\\clean_data_norm.csv'\n",
    "data = spark.read.csv(data_path, header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECK IF SPARK_DF GOOD TO GO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if df ok\n",
    "from pyspark.sql.functions import col, sum\n",
    "from pyspark.sql.functions import isnan, when, count\n",
    "\n",
    "null_counts = data.agg(*[sum(col(c).isNull().cast(\"int\")).alias(c) for c in data.columns])\n",
    "\n",
    "# Print out the null counts for each column\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pollutants=data.columns[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, format_number\n",
    "\n",
    "# assume 'data' is your Spark DataFrame\n",
    "means = data.agg(*[mean(c).alias(c) for c in data.columns[6:]])\n",
    "# Format the mean values to 4 decimal places\n",
    "formatted_means = means.select(*[format_number(c, 4).alias(c) for c in means.columns])\n",
    "\n",
    "# Show the formatted mean values\n",
    "formatted_means.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import stddev, format_number\n",
    "\n",
    "# assume 'data' is your Spark DataFrame\n",
    "stds = data.agg(*[stddev(c).alias(c) for c in data.columns[6:]])\n",
    "# Format the mean values to 4 decimal places\n",
    "formatted_stvs = stds.select(*[format_number(c, 4).alias(c) for c in stds.columns])\n",
    "\n",
    "# Show the formatted mean values\n",
    "formatted_stvs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the feature columns and target column from the spark_df dataframe\n",
    "feature_columns = ['BEN', 'EBE', 'CO', 'NMHC', 'NO_2', 'O_3', 'PM10', 'PM25', 'SO_2', 'TCH', 'TOL']\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "data = assembler.transform(data)\n",
    "data = data.select(col(\"AQI_GenPop_Index\").alias(\"label\"), col(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets using the pyspark.ml library\n",
    "train, test = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "#later: train=small df, test: big df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Bayesian logistic regression model in PyMC3\n",
    "n_features = len(feature_columns)\n",
    "with pm.Model() as AQI_model_spark:\n",
    "    # Priors for coefficients and bias, with better starting values\n",
    "    coeffs = pm.Normal(\"coeffs\", mu=0, sigma=1, shape=n_features, testval=np.zeros((n_features)))\n",
    "    bias = pm.Normal(\"bias\", mu=0, sigma=1)\n",
    "    \n",
    "    # Define the logistic function with added epsilon using theano.tensor as tt\n",
    "def logistic(x, epsilon=1e-6):\n",
    "   return 1 / (1 + tt.exp(-x)) + epsilon\n",
    "\n",
    "# Define the logistic function\n",
    "p = logistic(pm.math.dot(data.select(\"features\").rdd.flatMap(lambda x: x).collect(), coeffs) + bias)\n",
    "        \n",
    "# Define the Bernoulli likelihood\n",
    "y_obs = pm.Bernoulli(\"y_obs\", p=p, observed=data.select(\"label\").rdd.flatMap(lambda x: x).collect())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
