{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries as needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, first, last, lag, lead, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "conf = SparkConf().setAppName('yuck').setMaster(\"local[*]\").set(\"spark.driver.memory\", \"4g\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCALED VERSION WITH BIG DATA SET STARTS HERE\n",
    "\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "import os\n",
    "# Define the schema for the CSV files\n",
    "schema = StructType ([\n",
    " StructField ('date', StringType(), True ),\n",
    " StructField ('BEN', DoubleType(), True ),\n",
    " StructField ('CO', DoubleType(), True ),\n",
    " StructField ('EBE', DoubleType(), True ),\n",
    " StructField ('MXY', DoubleType(), True ),\n",
    " StructField ('NMHC', DoubleType(), True ),\n",
    " StructField ('NO_2', DoubleType(), True ),\n",
    " StructField ('NOx', DoubleType(), True ),\n",
    " StructField ('OXY', DoubleType(), True ),\n",
    " StructField ('O_3', DoubleType(), True ),\n",
    " StructField ('PM10', DoubleType(), True ),\n",
    " StructField ('PM25', DoubleType(), True ),\n",
    " StructField ('PXY', DoubleType(), True ),\n",
    " StructField ('SO_2', DoubleType(), True ),\n",
    " StructField ('TCH', DoubleType(), True ),\n",
    " StructField ('TOL', DoubleType(), True )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of CSV files to merge\n",
    "data_path = 'C:\\\\Users\\\\eleni\\\\Documents\\\\Diplw\\\\Jupyter-Notebooks\\\\diplw\\\\csvs_per_year'\n",
    "csv_files = [os.path.join(data_path,f) for f in os.listdir(data_path)]\n",
    "             #if f.endswith ('.csv ') and f.startswith('madrid_20')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files into a Spark DataFrame\n",
    "spark_df = spark.read.format('csv').schema(schema).option('header',True).load(csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the ’date’ column to Unix timestamps\n",
    "from pyspark.sql.functions import unix_timestamp, to_utc_timestamp\n",
    "from pyspark.sql.types import TimestampType\n",
    "spark_df = spark_df.withColumn('unix_time', unix_timestamp(spark_df.date ,'yyyy-MM-dd HH:mm:ss').cast('timestamp')).drop('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the DataFrame by unix_time and add a row number column\n",
    "window = Window.orderBy('unix_time')\n",
    "spark_df = spark_df.withColumn('row_num', row_number().over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILL IN MISSING VALUES STARTS HERE\n",
    "# Add previous and next value columns for each column except 'unix_time' and 'row_num'\n",
    "for col_name in spark_df.columns:\n",
    "    if col_name != \"unix_time\" and col_name != \"row_num\":\n",
    "        spark_df = spark_df.withColumn(f\"{col_name}_prev\", lag(col_name).over(window))\n",
    "        spark_df = spark_df.withColumn(f\"{col_name}_next\", lead(col_name).over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate missing values for each column except 'unix_time' and 'row_num'\n",
    "for col_name in spark_df.columns:\n",
    "    if col_name != \"unix_time\" and col_name != \"row_num\":\n",
    "        spark_df = spark_df.withColumn(col_name, when(col(col_name).isNull(),\n",
    "                                          (last(col_name, True).over(window) +\n",
    "                                           first(col_name, True).over(window)) / 2)\n",
    "                                          .otherwise(col(col_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns with previous and next value for each column\n",
    "for col_name in spark_df.columns:\n",
    "    if col_name.endswith(\"_prev\") or col_name.endswith(\"_next\"):\n",
    "        spark_df = spark_df.drop(col_name)\n",
    "#END FILL IN MISSING VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outlier Handling and normalization in apache spark\n",
    "\n",
    "from pyspark.ml.feature import RobustScaler\n",
    "# Get the columns to normalize\n",
    "pollutants = spark_df.columns[:-2]\n",
    "outliers = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pollutant in pollutants:\n",
    "    # Find outliers using the IQR method with k=1.5\n",
    "    quantiles = spark_df.approxQuantile(pollutant, [0.25, 0.75], 0.05)\n",
    "    Q1 = quantiles[0]\n",
    "    Q3 = quantiles[1]\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers[pollutant] = (lower_bound, upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_clean = spark_df.select(spark_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace outliers with null values\n",
    "for pollutant in pollutants:\n",
    "    # Rename the original column\n",
    "    spark_df_clean = spark_df_clean.withColumnRenamed(pollutant, f\"{pollutant}_orig\")\n",
    "\n",
    "    # Replace outliers with null values\n",
    "    expr = when(~col(f\"{pollutant}_orig\").between(outliers[pollutant][0], outliers[pollutant][1]), None).otherwise(col(f\"{pollutant}_orig\")).alias(pollutant)\n",
    "    spark_df_clean = spark_df_clean.select(\"*\", expr)\n",
    "    spark_df_clean = spark_df_clean.drop(f\"{pollutant}_orig\")\n",
    "    spark_df_clean = spark_df_clean.fillna({f\"{pollutant}\": \"null\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpolate->Fill null values of 'spark_df_clean'\n",
    "# Order the DataFrame by unix_time and add a row number column\n",
    "window_clean = Window.orderBy('unix_time')\n",
    "spark_df_clean = spark_df_clean.withColumn('row_num', row_number().over(window_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add previous and next value columns for each column except 'unix_time' and 'row_num'\n",
    "for col_name in spark_df_clean.columns:\n",
    "    if col_name != \"unix_time\" and col_name != \"row_num\":\n",
    "        spark_df_clean = spark_df_clean.withColumn(f\"{col_name}_prev\", lag(col_name).over(window_clean))\n",
    "        spark_df_clean = spark_df_clean.withColumn(f\"{col_name}_next\", lead(col_name).over(window_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate missing values for each column except 'unix_time' and 'row_num'\n",
    "for col_name in spark_df_clean.columns:\n",
    "    if col_name != \"unix_time\" and col_name != \"row_num\":\n",
    "        spark_df_clean = spark_df_clean.withColumn(col_name, when(col(col_name).isNull(),\n",
    "                                          (last(col_name, True).over(window_clean) +\n",
    "                                           first(col_name, True).over(window_clean)) / 2)\n",
    "                                          .otherwise(col(col_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns with previous and next value for each column\n",
    "for col_name in spark_df_clean.columns:\n",
    "    if col_name.endswith(\"_prev\") or col_name.endswith(\"_next\"):\n",
    "        spark_df_clean = spark_df_clean.drop(col_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if interpolation successful\n",
    "from pyspark.sql.functions import isnan, when, count\n",
    "\n",
    "# Count the number of null values in each column\n",
    "null_counts = spark_df_clean.select([count(when(isnan(c) | spark_df_clean[c].isNull(), c)).alias(c) for c in spark_df_clean.columns])\n",
    "\n",
    "# Show the results\n",
    "null_counts.show()\n",
    "#END FILL IN MISSING VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "\n",
    "# Assemble features into a single column\n",
    "assembler = VectorAssembler(inputCols=pollutants, outputCol=\"features\")\n",
    "spark_df_assembled = assembler.transform(spark_df_clean).select(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features using standard scaler\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withMean=True, withStd=True)\n",
    "#scaler_model = scaler.fit(spark_df_assembled)\n",
    "#spark_df_normalized = scaler_model.transform(spark_df_assembled).select(\"scaledFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leave this area for debugging, printing rows,values,columns etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample spark_df to visualize data\n",
    "\n",
    "sampled_data = spark_df.select('SO_2', 'NO_2', 'PM25', 'PM10', 'O_3').sample(False, 0.2) #20% of the data\n",
    "pandas_df = sampled_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert data type of pandas_df\n",
    "for col in pandas_df.columns:\n",
    "    # check if the column contains float64 data type\n",
    "    if pandas_df[col].dtype == np.float64:\n",
    "        # convert the column to float32 data type\n",
    "        pandas_df[col] = pandas_df[col].astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_per_column = pandas_df.memory_usage(deep=True)\n",
    "\n",
    "# get the total memory usage of the DataFrame\n",
    "total_memory = memory_per_column.sum()\n",
    "\n",
    "print(f\"Memory usage of DataFrame: {total_memory / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histograms for only selected pollutants\n",
    "pandas_df = pandas_df.reset_index(drop=True)\n",
    "\n",
    "# Create a 2x3 grid of subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(12, 8))\n",
    "\n",
    "# Loop through the pollutants and plot the histograms in each subplot\n",
    "for i, pollutant in enumerate(pandas_df.columns):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    ax = axes[row, col]\n",
    "    sns.histplot(data=pandas_df, x=pollutant, kde=True, color=\"purple\",bins=500, ax=ax)\n",
    "    ax.set_title(pollutant)\n",
    "    # get the range of values in the column, ignoring NaN and Inf\n",
    "    x_min = np.nanmin(pandas_df[pollutant][np.isfinite(pandas_df[pollutant])])\n",
    "    x_max = np.nanmax(pandas_df[pollutant][np.isfinite(pandas_df[pollutant])])\n",
    "\n",
    "    # set the x-axis limits\n",
    "    ax.set_xlim(x_min, x_max/1.5)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pathainei ligo kokobloko opote tha prospathisw na kanw histogram gia kathe column ksexwrista gamwt\n",
    "sns.histplot(data=pandas_df, x='PM25', kde=True, color='purple',bins=500)\n",
    "# set the x-axis and y-axis limits\n",
    "plt.xlim(0, 60)\n",
    "plt.ylim(0, 300000)\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of physical CPU cores: 4\n"
     ]
    }
   ],
   "source": [
    "#Checking dataframe size and cores to optimize partitions\n",
    "import psutil\n",
    "\n",
    "num_cores = psutil.cpu_count(logical=False)\n",
    "print(\"Number of physical CPU cores:\", num_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 10\n"
     ]
    }
   ],
   "source": [
    "#Check number of partitions\n",
    "num_partitions = spark_df.rdd.getNumPartitions()\n",
    "print(\"Number of partitions:\", num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.repartition(10)  # change 10 to the desired number of partitions\n",
    "print(spark_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark_df_clean.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pollutants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, col\n",
    "\n",
    "# Count the number of null values in the 'TOL' column of 'spark_df'\n",
    "tol_null_count = spark_df.filter(isnan(col('TOL'))).count()\n",
    "\n",
    "print(f\"The 'TOL' column of 'spark_df' has {tol_null_count} null values.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_clean.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnull\n",
    "\n",
    "# Count the number of null values in each column\n",
    "spark_df_clean.select([isnull(c).alias(c) for c in spark_df_clean.columns]).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.drop(spark_df.columns[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the merged DataFrame to a CSV file\n",
    "spark_df.coalesce(1).write.format('csv').option('header', True ).mode('overwrite').save('C:\\\\Users\\\\eleni\\\\Documents\\\\Diplw\\\\Jupyter-Notebooks\\\\diplw\\\\csvs_per_year\\\\yuck')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
