{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries as needed\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, first, last, lag, lead, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "conf = SparkConf().setAppName('yuck').setMaster(\"local[*]\").set(\"spark.driver.memory\", \"4g\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# Define the schema for the CSV files\n",
    "schema = StructType ([\n",
    " StructField ('date', StringType(), True ),\n",
    " StructField ('BEN', DoubleType(), True ),\n",
    " StructField ('CO', DoubleType(), True ),\n",
    " StructField ('EBE', DoubleType(), True ),\n",
    " StructField ('MXY', DoubleType(), True ),\n",
    " StructField ('NMHC', DoubleType(), True ),\n",
    " StructField ('NO_2', DoubleType(), True ),\n",
    " StructField ('NOx', DoubleType(), True ),\n",
    " StructField ('OXY', DoubleType(), True ),\n",
    " StructField ('O_3', DoubleType(), True ),\n",
    " StructField ('PM10', DoubleType(), True ),\n",
    " StructField ('PM25', DoubleType(), True ),\n",
    " StructField ('PXY', DoubleType(), True ),\n",
    " StructField ('SO_2', DoubleType(), True ),\n",
    " StructField ('TCH', DoubleType(), True ),\n",
    " StructField ('TOL', DoubleType(), True )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert csv files to spark_df dataframe\n",
    "data_path = 'C:\\\\Users\\\\eleni\\\\Documents\\\\Diplw\\\\Jupyter-Notebooks\\\\diplw\\\\csvs_per_year'\n",
    "spark_df = spark.read.csv(data_path, header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 1: Impute Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the ’date’ column to Unix timestamps\n",
    "from pyspark.sql.functions import unix_timestamp, to_utc_timestamp\n",
    "from pyspark.sql.types import TimestampType\n",
    "spark_df = spark_df.withColumn('unix_time', unix_timestamp(spark_df.date ,'yyyy-MM-dd HH:mm:ss').cast('timestamp')).drop('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the DataFrame by unix_time and add a row number column\n",
    "window = Window.orderBy('unix_time')\n",
    "spark_df = spark_df.withColumn('row_num', row_number().over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILL IN MISSING VALUES STARTS HERE\n",
    "# Add previous and next value columns for each column except 'unix_time' and 'row_num'\n",
    "for col_name in spark_df.columns:\n",
    "    if col_name != \"unix_time\" and col_name != \"row_num\":\n",
    "        spark_df = spark_df.withColumn(f\"{col_name}_prev\", lag(col_name).over(window))\n",
    "        spark_df = spark_df.withColumn(f\"{col_name}_next\", lead(col_name).over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate missing values for each column except 'unix_time' and 'row_num'\n",
    "from pyspark.sql.functions import col\n",
    "for col_name in spark_df.columns:\n",
    "    if col_name != \"unix_time\" and col_name != \"row_num\":\n",
    "        spark_df = spark_df.withColumn(col_name, when(col(col_name).isNull(),\n",
    "                                          (last(col_name, True).over(window) +\n",
    "                                           first(col_name, True).over(window)) / 2)\n",
    "                                          .otherwise(col(col_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns with previous and next value for each column\n",
    "for col_name in spark_df.columns:\n",
    "    if col_name.endswith(\"_prev\") or col_name.endswith(\"_next\"):\n",
    "        spark_df = spark_df.drop(col_name)\n",
    "#END FILL IN MISSING VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 2: ADD AQI COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADD AQI\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define the breakpoints and corresponding index levels for each pollutant\n",
    "breakpoints = {\n",
    "    'PM25': [0, 10, 20, 25, 50, 75, 800],\n",
    "    'PM10': [0, 20, 40, 50, 100, 150, 1200],\n",
    "    'NO_2': [0, 40, 90, 120, 230, 340, 1000],\n",
    "    'O_3': [0, 50, 100, 130, 240, 380, 800],\n",
    "    'SO_2': [0, 100, 200, 350, 500, 750, 1250]\n",
    "}\n",
    "# Define each category\n",
    "categories = ['Good', 'Fair', 'Moderate', 'Poor', 'Very Poor', 'Extremely Poor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the index level for each pollutant concentration\n",
    "from pyspark.sql.functions import array, array_max\n",
    "#import pyspark.sql.functions as F\n",
    "\n",
    "def calculate_index_level(pollutant, concentration):\n",
    "    breakpoints_list = breakpoints[pollutant]\n",
    "    for i in range(len(breakpoints_list)-1):\n",
    "        if breakpoints_list[i] <= concentration < breakpoints_list[i+1]:\n",
    "            return i+1\n",
    "    return 6  # If concentration exceeds the highest breakpoint, return the highest index level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "calculate_aqi_index_udf = udf(\n",
    "    lambda no2_conc, o3_conc, pm10_conc, pm25_conc, so2_conc:\n",
    "        int(max([calculate_index_level('NO_2', no2_conc),\n",
    "                 calculate_index_level('O_3', o3_conc),\n",
    "                 calculate_index_level('PM10', pm10_conc),\n",
    "                 calculate_index_level('PM25', pm25_conc),\n",
    "                 calculate_index_level('SO_2', so2_conc)])),\n",
    "    IntegerType()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Add AQI index as a new column to the DataFrame using the UDF\n",
    "spark_df = spark_df.withColumn('AQI_Index', calculate_aqi_index_udf('NO_2','O_3','PM10', 'PM25', 'SO_2'))\n",
    "\n",
    "# Define a UDF to calculate the AQI category for each row\n",
    "calculate_aqi_category_udf = udf(lambda index_level: categories[index_level-1], StringType())\n",
    "\n",
    "# Add AQI category as a new column to the DataFrame using the UDF\n",
    "spark_df = spark_df.withColumn('AQI_Category', calculate_aqi_category_udf('AQI_Index'))\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "#Create a new column named 'AQI_GenPop_Category'\n",
    "spark_df = spark_df.withColumn('AQI_GenPop_Category', when((spark_df['AQI_Index'] >= 1) & (spark_df['AQI_Index'] <= 3), 'Safe').otherwise('Hazardous'))\n",
    "\n",
    "#Create a new column named 'AQI_GenPop_Index'\n",
    "spark_df = spark_df.withColumn('AQI_GenPop_Index', when((spark_df['AQI_GenPop_Category'] == 'Safe'), 0).otherwise(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 3: REMOVE OUTLIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outlier Handling and normalization in apache spark\n",
    "\n",
    "from pyspark.ml.feature import RobustScaler\n",
    "# Get the columns to normalize\n",
    "pollutants = spark_df.columns[:-6]\n",
    "outliers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pollutants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pollutant in pollutants:\n",
    "    # Find outliers using the IQR method with k=1.5\n",
    "    quantiles = spark_df.approxQuantile(pollutant, [0.25, 0.75], 0.05)\n",
    "    Q1 = quantiles[0]\n",
    "    Q3 = quantiles[1]\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers[pollutant] = (lower_bound, upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_clean = spark_df.select(spark_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace outliers with null values\n",
    "from pyspark.sql.functions import col\n",
    "for pollutant in pollutants:\n",
    "    # Rename the original column\n",
    "    spark_df_clean = spark_df_clean.withColumnRenamed(pollutant, f\"{pollutant}_orig\")\n",
    "\n",
    "    # Replace outliers with null values\n",
    "    expr = when(~col(f\"{pollutant}_orig\").between(outliers[pollutant][0], outliers[pollutant][1]), None).otherwise(col(f\"{pollutant}_orig\")).alias(pollutant)\n",
    "    spark_df_clean = spark_df_clean.select(\"*\", expr)\n",
    "    spark_df_clean = spark_df_clean.drop(f\"{pollutant}_orig\")\n",
    "    spark_df_clean = spark_df_clean.fillna({f\"{pollutant}\": \"null\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpolate->Fill null values of 'spark_df_clean'\n",
    "# Order the DataFrame by unix_time and add a row number column\n",
    "window_clean = Window.orderBy('unix_time')\n",
    "spark_df_clean = spark_df_clean.withColumn('row_num', row_number().over(window_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add previous and next value columns for each column except 'unix_time' and 'row_num'\n",
    "for col_name in spark_df_clean.columns:\n",
    "    if col_name != \"unix_time\" and col_name != \"row_num\" and col_name != \"AQI_Index\" and col_name != \"AQI_Category\" and col_name != \"AQI_GenPop_Index\" and col_name != \"AQI_GenPop_Category\":\n",
    "        spark_df_clean = spark_df_clean.withColumn(f\"{col_name}_prev\", lag(col_name).over(window_clean))\n",
    "        spark_df_clean = spark_df_clean.withColumn(f\"{col_name}_next\", lead(col_name).over(window_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate missing values for each column except 'unix_time' and 'row_num'\n",
    "for col_name in spark_df_clean.columns:\n",
    "    if col_name != \"unix_time\" and col_name != \"row_num\" and col_name != \"AQI_Index\" and col_name != \"AQI_Category\" and col_name != \"AQI_GenPop_Index\" and col_name != \"AQI_GenPop_Category\":\n",
    "        spark_df_clean = spark_df_clean.withColumn(col_name, when(col(col_name).isNull(),(last(col_name, True).over(window_clean) + first(col_name, True).over(window_clean)) / 2).otherwise(col(col_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns with previous and next value for each column\n",
    "for col_name in spark_df_clean.columns:\n",
    "    if col_name.endswith(\"_prev\") or col_name.endswith(\"_next\"):\n",
    "        spark_df_clean = spark_df_clean.drop(col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PXY has 100 null values after interpolation, i will fill those seperately\n",
    "from pyspark.sql.functions import mean\n",
    "\n",
    "# Calculate the mean value of the 'PXY' column\n",
    "mean_val = spark_df_clean.select(mean(spark_df_clean['PXY'])).collect()[0][0]\n",
    "\n",
    "# Fill the remaining null values in the 'PXY' column with the mean value\n",
    "spark_df_clean = spark_df_clean.fillna(mean_val, subset=['PXY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if interpolation successful\n",
    "from pyspark.sql.functions import col, sum\n",
    "from pyspark.sql.functions import isnan, when, count\n",
    "\n",
    "null_counts = spark_df_clean.agg(*[sum(col(c).isNull().cast(\"int\")).alias(c) for c in spark_df_clean.columns])\n",
    "\n",
    "# Print out the null counts for each column\n",
    "null_counts.show()\n",
    "\n",
    "#END FILL IN MISSING VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 4: Z-SCORE NORMALIZATION FOR MEAN=0 and STD=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.functions import array\n",
    "\n",
    "# Get the columns to normalize\n",
    "pollutants = spark_df_clean.columns[6:]\n",
    "\n",
    "# Create a VectorAssembler to combine the columns to be normalized\n",
    "assembler = VectorAssembler(inputCols=pollutants, outputCol=\"features\")\n",
    "\n",
    "# Transform the Spark DataFrame using the VectorAssembler\n",
    "assembled_df = assembler.transform(spark_df_clean)\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=True)\n",
    "\n",
    "# Compute summary statistics and generate a StandardScalerModel\n",
    "scalerModel = scaler.fit(assembled_df)\n",
    "\n",
    "# Normalize the data using the StandardScalerModel\n",
    "scaled_df = scalerModel.transform(assembled_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'scaledFeatures' struct column into an array column\n",
    "array_df = scaled_df.select(\"row_num\", vector_to_array('scaledFeatures').alias('scaled_array'))\n",
    "\n",
    "# Select individual elements of the array and create separate columns for each pollutant\n",
    "for i, col in enumerate(pollutants):\n",
    "    array_df = array_df.withColumn(col, array_df['scaled_array'][i])\n",
    "\n",
    "# Drop the 'scaled_array' column\n",
    "array_df = array_df.drop('scaled_array')\n",
    "\n",
    "# Join the normalized pollutant columns with the original DataFrame\n",
    "norm_df = spark_df_clean.select(*spark_df_clean.columns[:6]).join(array_df, on=\"row_num\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df.filter(\"row_num in (1, 2, 3, 4, 5)\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leave this area for debugging, printing rows,values,columns etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample spark_df to visualize data\n",
    "\n",
    "sampled_data = spark_df_clean.sample(False, 0.1) #40% of the data\n",
    "pandas_df = sampled_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top = pandas_df[['CO', 'EBE','PXY','NO_2', 'O_3', 'PM10', 'PM25','TCH']]\n",
    "df_bottom = pandas_df[['BEN', 'MXY', 'NMHC', 'NOx', 'OXY', 'SO_2', 'TOL']]\n",
    "\n",
    "fig, axs = plt.subplots(2, figsize=(10, 8))\n",
    "\n",
    "# First plot with first 7 rows\n",
    "sns.boxplot(data=df_top, palette='PuRd', ax=axs[0])\n",
    "axs[0].set_xticklabels(df_top.columns, rotation=45)\n",
    "\n",
    "# Second plot with last 7 rows\n",
    "sns.boxplot(data=df_bottom, palette='PuRd', ax=axs[1])\n",
    "axs[1].set_xticklabels(df_bottom.columns, rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = pandas_df.reset_index(drop=True)\n",
    "\n",
    "# Create a 2x3 grid of subplots\n",
    "fig, axs = plt.subplots(nrows=5, ncols=3, figsize=(12, 12))\n",
    "\n",
    "# Plot a histogram for each column\n",
    "for i, pollutant in enumerate(pollutants):\n",
    "    # Get the data for the column\n",
    "    data = pandas_df[pollutant].dropna()\n",
    "    \n",
    "    # Determine the subplot location based on the column index\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    # get the range of values in the column, ignoring NaN and Inf\n",
    "    x_min = np.nanmin(pandas_df[pollutant][np.isfinite(pandas_df[pollutant])])\n",
    "    x_max = np.nanmax(pandas_df[pollutant][np.isfinite(pandas_df[pollutant])])\n",
    "    \n",
    "    # calculate number of bins using IQR rule\n",
    "    n = len(pandas_df[pollutant])\n",
    "    std = np.std(pandas_df[pollutant])\n",
    "    k = 3.5 * std / (n**(1/3))\n",
    "    num_bins = int(np.ceil((x_max - x_min) / k)) if k != 0 else 1\n",
    "    # Plot the histogram with kde\n",
    "    sns.histplot(data, kde=True, bins=num_bins, color=sns.color_palette(\"PuRd\", 15)[i], ax=axs[row, col])\n",
    "    \n",
    "    # Add a vertical line for the mean\n",
    "    mean = data.mean()\n",
    "    axs[row, col].axvline(mean, color='k', linestyle='dashed', linewidth=1)\n",
    "    \n",
    "    # Add a vertical line for the standard deviation\n",
    "    std = data.std()\n",
    "    axs[row, col].axvline(mean+std, color='#8C78F0', linestyle='dashed', linewidth=1)\n",
    "    axs[row, col].axvline(mean-std, color='#8C78F0', linestyle='dashed', linewidth=1)\n",
    "    \n",
    "    # Set the title and axis labels\n",
    "    axs[row, col].set_title(pollutant)\n",
    "    axs[row, col].set_xlabel('Value')\n",
    "    axs[row, col].set_ylabel('Density')\n",
    "    axs[row, col].set_xlim([data.min(), data.max()])  # set x-axis range\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pollutants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas df of AQI_Category\n",
    "\n",
    "sampled_data = spark_df_clean.select('AQI_Category')\n",
    "pdf = sampled_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the count of each category and calculate the percentage\n",
    "category_count = pdf.groupby('AQI_Category').size()\n",
    "category_percentage = category_count / category_count.sum() * 100\n",
    "\n",
    "\n",
    "# Create the bar plot\n",
    "ax = sns.barplot(x=category_percentage.index, y=category_percentage.values, palette=\"BuPu\")\n",
    "\n",
    "# Set the axis labels and title\n",
    "ax.set(xlabel='AQI Category', ylabel='Percentage', title='Percentage of AQI Categories')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_clean.write.format('csv').option('header', True ).mode('overwrite').save('C:/Users/eleni/Documents/Diplw/Jupyter-Notebooks/diplw/csvs_per_year/clean_wo_norm.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file in the current working directory\n",
    "pandas_df.to_csv('C:\\\\Users\\\\eleni\\\\Documents\\\\Diplw\\\\Jupyter-Notebooks\\\\diplw\\\\clean_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
